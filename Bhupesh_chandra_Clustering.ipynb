
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score
import seaborn as sns
import matplotlib.pyplot as plt

# Load datasets
customers = pd.read_csv("Customers.csv")
transactions = pd.read_csv("Transactions.csv")

# Aggregate customer transaction data
data = transactions.groupby("CustomerID").agg({
    "TotalValue": "sum",
    "Quantity": "sum",
    "TransactionID": "count"
}).reset_index()

# Merge with customer profiles
data = data.merge(customers, on="CustomerID")

# One-hot encoding for region
encoded_data = pd.get_dummies(data[["Region"]])
features = pd.concat([data[["TotalValue", "Quantity", "TransactionID"]], encoded_data], axis=1)

# Scaling the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

# Determine the optimal number of clusters using Davies-Bouldin Index
db_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(scaled_features)
    db_score = davies_bouldin_score(scaled_features, labels)
    db_scores.append(db_score)

# Optimal number of clusters
optimal_k = db_scores.index(min(db_scores)) + 2
print(f"Optimal Number of Clusters: {optimal_k}, DB Index: {min(db_scores)}")

# Final clustering
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
data["Cluster"] = kmeans.fit_predict(scaled_features)

# Save results
data.to_csv("Bhupesh_chandra_Clustering.csv", index=False)

# Visualization
sns.scatterplot(x=data["TotalValue"], y=data["Quantity"], hue=data["Cluster"], palette="viridis")
plt.title("Customer Clusters")
plt.show()
